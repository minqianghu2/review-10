Merits:

The expressiveness make deep neural networks successful. 

There is no distinct differences between individual high level units and random linear combinations of high level units. At the same time, the deep neural networks learn input and output mapping that are fairly discontinuous to a significant extent. 

In MNIST dataset, some architectures such as a simple fully connected network with one or more hidden layers and a Softmax classifier are used. The ImageNet dataset is used as well, which is also known as Krizhevsky et. al architecture.

In this paper, the deep neural networks’ counter intuition has been demonstrated, with respections both to the semantic meanings and their discontinuities.
Shortcomings:

Many unexpected and counter-intuitive properties are caused by the uninterpretable solutions.

There are still some blind spots in neural networks. The current unit-level inspection has very few connections beyond confirming certain intuition regarding the complexity of the representations.
